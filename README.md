## This is My Web Scraping Projects Repository
---
### **Description:** 
I have completed 8 Web Scraping Projects in which it goes from Minor Projects of Scraping to Major Projects of Automation. I have done scraping projects in Python using the libraries *BeautifulSoup* and *Selenium*.
***
### **Libraries Used:**

| Name       |  Used         | 
| ------------- |:-------------:|
| `BeautifulSoup` |  *for extracting data from HTML websites.* |
| `Selenium`      |    *as web automation tool for scraping, testing, and browser automation.*   |
| `Pandas`        |  *for creating dataframe and importing data in csv files*    | 
| `smtplib, ssl`  |  *for sending emails programmatically over SMTP with SSL/TLS.*       |
| `Mime`          |     *for creating email messages with attachments.*     |
___
### **Projects:**
#### 1. [Stocks Scraping](Jupyter%20Files/Project1_Stocks_Scraping.ipynb) : (Basic)
   * I have Scraped certain contents from the stocks website about Apple stocks price.
#### 2. [National Football League (NFL) Table Scraping](Jupyter%20Files/Project2_Football_League_(NFL)_Scraping.ipynb) : (Basic)
   * I have Scraped NFL website table data and saved it in a CSV file.
#### 3. [Car Prices Scraping](Jupyter%20Files/Project3_Car_Prices_Scraping.ipynb) : (Basic)
   * I have Scraped cars website cars details of multiple pages in a single click.
   * For moving on multiple pages one by one I used next-page links in a loop.
#### 4. [IMDB Movies feature Scraping](Jupyter%20Files/Project4_IMDB_Ratings_by_Selenium.ipynb) : (Basic)
   * I have used Selenium for navigating through buttons and moving on pages.
   * Features of the movie *The Prestige* I have captured by using the screenshot function.
   * Infinite Scrolling of pages is done using the function to move to certain heights at a time.
#### 5. [Nike Sale Products Price Scraping](Jupyter%20Files/Project5_Nike%20Sale%20Product%20Prices_Scraping.ipynb) : [Major]
   * I have Scraped the data of Nike sale page website in which link of thr product, Name, prices are extracted from it.
   * Infinite Scrolling function is used in this website as we move down new products are updated.
   * Apart from capturing details I have stored the data in a DataFrame using Pandas library and then stored it in a CSV file.
#### 6. [Twitter Tweets Scraping](Jupyter%20Files/Project6_Twitter%20Tweets_Scraping.ipynb) : [Major]
   * I have Scraped 200+ tweets of PM Modi using Selenium as Twitter is Infinite Scrolling Website.
   * I have used buttons and navigation functions to automatically move and log into the website.
   * WaitTime function is used to slow down the process for a second to load the whole page.
   * Tweets are stored in DataFrame which is then imported to a CSV file.
#### 7. [Indeed Jobs Scraping](Jupyter%20Files/Project7_IndeedJobs_Scraping.ipynb) : [Major]
   * I have Scraped more than 1000 jobs in a minute and stored the data of Job link, Postion, Company Name and Date Posted.
   * Pages are scraped one by one using loop and captures the jobs data of each page.
   * Every Data is captured in DataFrame which is then imported to CSV file.
#### 8. [Jobs Data Automation on Email](Jupyter%20Files/Project8_Indeed%20Jobs%20Automation%20on%20Email.ipynb) : [Major]
   * This Projects Captures the data of your job preference and then it saves it into a csv file.
   * After the process of Scraping end it will send the CSV file to your email.
   * The whole process works on a single click of a button.
---
#### **Connect with me:** 
<p>
 <a href="https://www.linkedin.com/in/sauravsinghhhh/" target="_blank" rel="noreferrer"><img src="https://github.com/saurav2021c/Portfolio-project/blob/master/src/images/LinkedIN_black.png" width="35" /></a>
</p>


